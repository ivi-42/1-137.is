<!DOCTYPE html>
<html lang="en-us">
<head>
  <link rel="preload" href="/lib/font-awesome/webfonts/fa-brands-400.woff2" as="font" type="font/woff2" crossorigin="anonymous">
  <link rel="preload" href="/lib/font-awesome/webfonts/fa-regular-400.woff2" as="font" type="font/woff2" crossorigin="anonymous">
  <link rel="preload" href="/lib/font-awesome/webfonts/fa-solid-900.woff2" as="font" type="font/woff2" crossorigin="anonymous">
  <link rel="preload" href="/lib/JetBrainsMono/web/woff2/JetBrainsMono-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
  <script type="text/javascript" src="https://latest.cactus.chat/cactus.js"></script>
  <link rel="stylesheet" href="https://latest.cactus.chat/style.css" type="text/css">
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title> Hello_world | 137.is</title>
  <link rel = 'canonical' href = 'https://137.is/blog/ppo/'>
  <meta name="description" content="137.is a research project created to study non-linear systems, DAOs, and cybernetics.">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="robots" content="all,follow">
  <meta name="googlebot" content="index,follow,snippet,archive">
  <meta property="og:title" content="Hello_world" />
<meta property="og:description" content="PPO" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://137.is/blog/ppo/" /><meta property="article:section" content="blog" />
<meta property="article:published_time" content="2022-12-27T15:17:02+01:00" />
<meta property="article:modified_time" content="2022-12-27T15:17:02+01:00" />

  <meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Hello_world"/>
<meta name="twitter:description" content="PPO"/>

  
  
    
  
  
  <link rel="stylesheet" href="https://137.is/css/styles.c05d68261bf086a9d7713c4f8a6215a3601608e267a816a7ee58f139b3d1aae51222aae2081c8e0c6bd35e1334773b7a16283022f31f92afd93bb37e5e822e66.css" integrity="sha512-wF1oJhvwhqnXcTxPimIVo2AWCOJnqBan7ljxObPRquUSIqriCByODGvTXhM0dzt6FigwIvMfkq/ZO7N&#43;XoIuZg=="> 

  
  
  
    <!--[if lt IE 9]>
      <script src="https://oss.maxcdn.com/html5shiv/3.7.2/html5shiv.min.js"></script>
      <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
    <![endif]-->
  

  
<link rel="icon" type="image/png" href="https://137.is/images/favicon.ico" />


  
  
  
  
</head>

<body class="max-width mx-auto px3 ltr">
  <div class="content index py4">

    <header id="header">
  <a href="https://137.is/">
  
    <div id="logo" style="background-image: url(https://137.is/images/logo.png)"></div>
  
  <div id="title">
    <h1>137.is</h1>
  </div>
  </a>
  <div id="nav">
    <ul>
      <li class="icon">
        <a href="#" aria-label="Menu"><i class="fas fa-bars fa-2x" aria-hidden="true"></i></a>
      </li>
      
        <li><a href="/">Home</a></li>
      
        <li><a href="/posts">Posts</a></li>
      
        <li><a href="/tags">Tags</a></li>
      
        <li><a href="/about">About</a></li>
      
    </ul>
  </div>
</header>



    
<article class="post" itemscope itemtype="http://schema.org/BlogPosting">

  <div class="content" itemprop="articleBody">
  
    <h1 id="ppo">PPO</h1>
<p>I am doing the hugging face Reinforcement Learning Course. It is quite nice, and I though I was going to put some thoughts.</p>
<p>The first corse parameters is about PPO.</p>
<p>Proximal Policy Optimization (PPO) is an algorithm for training reinforcement learning (RL) agents. RL is a type of machine learning where an agent learns to take actions in an environment to maximize a reward signal.</p>
<p>The goal of PPO is to improve the performance of an RL agent by making small, incremental updates to its policy (the strategy the agent uses to choose actions). To do this, PPO uses a technique called &ldquo;policy gradient,&rdquo; which involves estimating the gradient of the expected reward with respect to the parameters of the policy, and using this gradient to update the policy parameters in the direction that is expected to increase the reward.</p>
<p>PPO has several key features that make it an effective algorithm for training RL agents:</p>
<p>It is a &ldquo;trust region&rdquo; method, which means that it limits the size of the policy updates to prevent the agent from straying too far from its current policy. This helps to stabilize the learning process and prevent the agent from learning suboptimal policies.</p>
<p>It uses a &ldquo;clip objective,&rdquo; which means that it clips the probability of actions taken by the agent to keep them within a certain range. This helps to prevent the agent from taking actions that are too risky or extreme, which can lead to poor performance.</p>
<p>It is computationally efficient and can be implemented with a relatively simple neural network architecture.</p>
<p>Overall, PPO is a widely used and effective algorithm for training RL agents, and has been applied to a variety of tasks, including playing games, controlling robots, and optimizing industrial processes.</p>

  
  </div>
</article>


    <footer id="footer">
  <div class="footer-left">
    Copyright  &copy; 2022  137.is 
  </div>
  <div class="footer-right">
    <nav>
      <ul>
         
        <li><a href="/">Home</a></li>
         
        <li><a href="/posts">Posts</a></li>
         
        <li><a href="/tags">Tags</a></li>
         
        <li><a href="/about">About</a></li>
        
      </ul>
    </nav>
  </div>
</footer>


  </div>
</body>

<link rel="stylesheet" href=/lib/font-awesome/css/all.min.css>
<script src=/lib/jquery/jquery.min.js></script>
<script src=/js/main.js></script>
</html>
